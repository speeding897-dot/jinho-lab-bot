import os
import requests
from bs4 import BeautifulSoup
import json
import random

# ==========================================
# 1. ì„¤ì • (ê²€ìƒ‰ì–´ ì—†ìŒ! ê·œëª¨ë³„ URL íƒ€ê²ŸíŒ…)
# ==========================================
# ì¸í¬ë£¨íŠ¸ ê¸°ì—…ë¶„ë¥˜ë³„ URL ë¦¬ìŠ¤íŠ¸ (ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…, ê°•ì†Œê¸°ì—…)
TARGET_URLS = [
    # ëŒ€ê¸°ì—… (Group & Big)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=1", 
    # ì¤‘ê²¬ê¸°ì—… (Mid-sized)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=2", 
    # ê°•ì†Œê¸°ì—…/íˆë“ ì±”í”¼ì–¸ (Hidden Champion)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=3"
]

# ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

# í˜¹ì‹œ ì„ì—¬ ìˆì„ ê³µê¸°ì—…/ê³µë¬´ì› í•„í„°ë§
EXCLUDE_KEYWORDS = ["ê³µì‚¬", "ê³µë‹¨", "ì¬ë‹¨", "í˜‘íšŒ", "ì§„í¥ì›", "ì‹œì²­", "êµ¬ì²­", "ì„¼í„°", "ê³µë¬´ì›", "ë³´ê±´ì†Œ"]

def collect_private_jobs_by_size():
    print(f"ğŸ”¥ [ì¸í¬ë£¨íŠ¸] ê¸°ì—… ê·œëª¨ë³„(ëŒ€/ì¤‘ê²¬/ê°•ì†Œ) ì•Œì§œë°°ê¸° ê³µê³  ìˆ˜ì§‘ ì‹œì‘...")
    
    total_jobs = []
    
    # ê° ê¸°ì—… ê·œëª¨ë³„ í˜ì´ì§€ë¥¼ ëŒë©´ì„œ ë°ì´í„°ë¥¼ ëª¨ìŒ
    for url in TARGET_URLS:
        if len(total_jobs) >= 30: break # 30ê°œ ì°¨ë©´ ì¤‘ë‹¨
        
        try:
            print(f"   Targeting URL: {url}...")
            response = requests.get(url, headers=HEADERS, timeout=10)
            
            # ì¸ì½”ë”© ìë™ ê°ì§€ (í•œê¸€ ê¹¨ì§ ë°©ì§€)
            response.encoding = response.apparent_encoding 

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì¸í¬ë£¨íŠ¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ íƒìƒ‰
            all_list_items = soup.find_all('li')
            
            # í˜ì´ì§€ë‹¹ ìµœëŒ€ 10ê°œì”©ë§Œ ë½‘ì•„ì„œ ì„ê¸° (ë‹¤ì–‘ì„±ì„ ìœ„í•´)
            count_per_page = 0
            
            for item in all_list_items:
                if len(total_jobs) >= 30: break
                if count_per_page >= 10: break # í•œ ì¹´í…Œê³ ë¦¬ë‹¹ 10ê°œë§Œ (ê³¨ê³ ë£¨ ìˆ˜ì§‘)

                try:
                    # 1. íšŒì‚¬ëª… ì¶”ì¶œ & í•„í„°ë§
                    comp_tag = item.find(class_='cpname')
                    if not comp_tag: continue
                    
                    company = comp_tag.get_text(strip=True)

                    # ê³µê¸°ì—… í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€ (ìˆœìˆ˜ ì‚¬ê¸°ì—…ë§Œ)
                    if any(k in company for k in EXCLUDE_KEYWORDS):
                        continue

                    # 2. ì œëª© & ë§í¬ ì¶”ì¶œ
                    title_tag = item.find(class_='cl_top') or item.find(class_='hdit')
                    if not title_tag: continue
                    
                    link_tag = title_tag.find('a')
                    if not link_tag: continue

                    title = link_tag.get_text(strip=True)
                    link = link_tag['href']
                    
                    # ë§í¬ ì ˆëŒ€ê²½ë¡œ ë³€í™˜
                    if link.startswith("/"):
                        link = "https://job.incruit.com" + link

                    # 3. ê¸°ì—… í˜•íƒœ(ê·œëª¨) íƒœê·¸ ì¶”ì¶œ
                    type_tags = []
                    for icon in item.find_all(class_='icon'):
                        tag_text = icon.get_text(strip=True)
                        if tag_text: type_tags.append(tag_text)
                    
                    # URLì— ë”°ë¼ ê°•ì œ íƒœê·¸ ë¶€ì—¬ (ë°ì´í„°ê°€ ë¹„ì–´ìˆì„ ê²½ìš° ëŒ€ë¹„)
                    if "cd=1" in url and "ëŒ€ê¸°ì—…" not in type_tags: type_tags.insert(0, "ëŒ€ê¸°ì—…")
                    elif "cd=2" in url and "ì¤‘ê²¬ê¸°ì—…" not in type_tags: type_tags.insert(0, "ì¤‘ê²¬ê¸°ì—…")
                    elif "cd=3" in url and "ê°•ì†Œê¸°ì—…" not in type_tags: type_tags.insert(0, "ê°•ì†Œê¸°ì—…")
                    
                    type_str = ", ".join(type_tags)

                    # 4. ì„¸ë¶€ ì •ë³´ (ê²½ë ¥, í•™ë ¥ ë“±)
                    details = item.find_all('span')
                    exp = "ë¬´ê´€"
                    edu = "ë¬´ê´€"
                    
                    info_texts = [d.get_text(strip=True) for d in details if len(d.get_text(strip=True)) > 1]
                    for text in info_texts:
                        if "ê²½ë ¥" in text or "ì‹ ì…" in text: exp = text
                        elif "ëŒ€ì¡¸" in text or "ê³ ì¡¸" in text or "í•™ë ¥" in text: edu = text

                    # 5. ë§ˆê°ì¼ ì¶”ì¶œ
                    deadline_tag = item.find(class_='cl_btm')
                    deadline = "ì±„ìš©ì‹œ"
                    if deadline_tag:
                        d_span = deadline_tag.find('span')
                        if d_span: deadline = d_span.get_text(strip=True)

                    # 6. ë°ì´í„° ë‹´ê¸°
                    job_data = {
                        "id": len(total_jobs) + 1,
                        "company": company,
                        "type": type_str,
                        "title": title,
                        "exp": exp,
                        "edu": edu,
                        "deadline": deadline,
                        "link": link
                    }
                    
                    total_jobs.append(job_data)
                    count_per_page += 1
                    print(f"      âœ… [{len(total_jobs)}] {company} ({type_str})")

                except Exception:
                    continue 

        except Exception as e:
            print(f"   âŒ URL ì ‘ì† ì˜¤ë¥˜: {e}")
            continue

    return total_jobs

# --- ì‹¤í–‰ ë° íŒŒì¼ ì €ì¥ ---
if __name__ == "__main__":
    jobs = collect_private_jobs_by_size()
    
    if jobs:
        # 1. JOBS í´ë” ìƒì„±
        save_dir = "JOBS"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        # 2. íŒŒì¼ ì €ì¥
        save_path = os.path.join(save_dir, "recruit_data.json")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=4)
            
        print("\n" + "="*50)
        print(f"ğŸ‰ ê¸°ì—…ê·œëª¨ë³„ ì•Œì§œ ê³µê³  {len(jobs)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}")
        print("="*50)
    else:
        print("\nğŸ’€ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")