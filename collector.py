import os
import requests
from bs4 import BeautifulSoup
import json
import random
import time

# ==========================================
# 1. ì„¤ì • (ê²€ìƒ‰ì–´ ì—†ìŒ! ê·œëª¨ë³„ URL íƒ€ê²ŸíŒ…)
# ==========================================
# ì¸í¬ë£¨íŠ¸ ê¸°ì—…ë¶„ë¥˜ë³„ URL ë¦¬ìŠ¤íŠ¸ (ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…, ê°•ì†Œê¸°ì—…)
TARGET_URLS = [
    # ëŒ€ê¸°ì—… (Group & Big)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=1", 
    # ì¤‘ê²¬ê¸°ì—… (Mid-sized)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=2", 
    # ê°•ì†Œê¸°ì—…/íˆë“ ì±”í”¼ì–¸ (Hidden Champion)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=3"
]

# ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

# í˜¹ì‹œ ì„ì—¬ ìˆì„ ê³µê¸°ì—…/ê³µë¬´ì› í•„í„°ë§
EXCLUDE_KEYWORDS = ["ê³µì‚¬", "ê³µë‹¨", "ì¬ë‹¨", "í˜‘íšŒ", "ì§„í¥ì›", "ì‹œì²­", "êµ¬ì²­", "ì„¼í„°", "ê³µë¬´ì›", "ë³´ê±´ì†Œ"]

# â˜… ìµœì¢…ì ìœ¼ë¡œ ì €ì¥í•  ê³µê³  ê°œìˆ˜ ëª©í‘œ
FINAL_TARGET_COUNT = 30

def collect_private_jobs_by_size():
    print(f"ğŸ”¥ [ì¸í¬ë£¨íŠ¸] ê´‘ë²”ìœ„ ìˆ˜ì§‘ ëª¨ë“œ ê°€ë™ (ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 200ê°œ íƒìƒ‰)...")
    
    candidate_jobs = [] # í›„ë³´êµ°ì„ ë‹´ì„ ì„ì‹œ ë¦¬ìŠ¤íŠ¸
    
    # ê° ê¸°ì—… ê·œëª¨ë³„ í˜ì´ì§€ë¥¼ ëŒë©´ì„œ ë°ì´í„°ë¥¼ ëª¨ìŒ
    for url in TARGET_URLS:
        try:
            print(f"   Targeting URL: {url}...")
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.encoding = response.apparent_encoding 
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì¸í¬ë£¨íŠ¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì •ë°€ íƒ€ê²ŸíŒ…
            all_list_items = soup.select('ul.c_list > li')
            
            # [ìˆ˜ì •] ì¹´í…Œê³ ë¦¬ë‹¹ íƒìƒ‰ í•œë„ë¥¼ 200ê°œë¡œ ëŒ€í­ ìƒí–¥ (ì†Œì¥ë‹˜ ì§€ì‹œ ë°˜ì˜)
            count_per_category = 0
            
            for item in all_list_items:
                if count_per_category >= 200: break # ì¹´í…Œê³ ë¦¬ë‹¹ 200ê°œê¹Œì§€ë§Œ í™•ì¸

                try:
                    # 1. íšŒì‚¬ëª… ì¶”ì¶œ & í•„í„°ë§
                    comp_tag = item.find(class_='cpname')
                    if not comp_tag: continue
                    company = comp_tag.get_text(strip=True)

                    # ê³µê¸°ì—… í‚¤ì›Œë“œ í•„í„°ë§
                    if any(k in company for k in EXCLUDE_KEYWORDS):
                        continue

                    # 2. ì œëª© & ë§í¬ ì¶”ì¶œ
                    title_tag = item.find(class_='cl_top') or item.find(class_='hdit')
                    if not title_tag: continue
                    
                    link_tag = title_tag.find('a')
                    if not link_tag: continue

                    title = link_tag.get_text(strip=True)
                    link = link_tag['href']
                    if link.startswith("/"):
                        link = "https://job.incruit.com" + link

                    # 3. ê¸°ì—… í˜•íƒœ(ê·œëª¨) íƒœê·¸ ì¶”ì¶œ
                    type_tags = []
                    for icon in item.find_all(class_='icon'):
                        tag_text = icon.get_text(strip=True)
                        if tag_text: type_tags.append(tag_text)
                    
                    if "cd=1" in url and "ëŒ€ê¸°ì—…" not in type_tags: type_tags.insert(0, "ëŒ€ê¸°ì—…")
                    elif "cd=2" in url and "ì¤‘ê²¬ê¸°ì—…" not in type_tags: type_tags.insert(0, "ì¤‘ê²¬ê¸°ì—…")
                    elif "cd=3" in url and "ê°•ì†Œê¸°ì—…" not in type_tags: type_tags.insert(0, "ê°•ì†Œê¸°ì—…")
                    
                    type_str = ", ".join(type_tags)

                    # 4. ì„¸ë¶€ ì •ë³´ (ê²½ë ¥, í•™ë ¥ ë“±)
                    details = item.find_all('span')
                    exp = "ë¬´ê´€"
                    edu = "ë¬´ê´€"
                    
                    info_texts = [d.get_text(strip=True) for d in details if len(d.get_text(strip=True)) > 1]
                    for text in info_texts:
                        if "ê²½ë ¥" in text or "ì‹ ì…" in text: exp = text
                        elif "ëŒ€ì¡¸" in text or "ê³ ì¡¸" in text or "í•™ë ¥" in text: edu = text

                    # 5. ë§ˆê°ì¼ ì¶”ì¶œ
                    deadline_tag = item.find(class_='cl_btm')
                    deadline = "ì±„ìš©ì‹œ"
                    if deadline_tag:
                        d_span = deadline_tag.find('span')
                        if d_span: deadline = d_span.get_text(strip=True)

                    # 6. ë°ì´í„° ë‹´ê¸° (IDëŠ” ë‚˜ì¤‘ì— ë¶€ì—¬)
                    job_data = {
                        "company": company,
                        "type": type_str,
                        "title": title,
                        "exp": exp,
                        "edu": edu,
                        "deadline": deadline,
                        "link": link
                    }
                    
                    candidate_jobs.append(job_data)
                    count_per_category += 1
                    # ë¡œê·¸ëŠ” ë„ˆë¬´ ë§ì´ ëœ¨ë©´ ì§€ì €ë¶„í•˜ë‹ˆ 10ê°œ ë‹¨ìœ„ë¡œë§Œ ì¶œë ¥
                    if count_per_category % 10 == 0:
                        print(f"      ...í˜„ì¬ ì¹´í…Œê³ ë¦¬ì—ì„œ {count_per_category}ê°œ í™•ë³´ ì¤‘")

                except Exception:
                    continue 
            
            time.sleep(1) # ì°¨ë‹¨ ë°©ì§€

        except Exception as e:
            print(f"   âŒ URL ì ‘ì† ì˜¤ë¥˜: {e}")
            continue

    print(f"\nğŸ“Š 1ì°¨ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(candidate_jobs)}ê°œì˜ í›„ë³´ ê³µê³  í™•ë³´")
    
    # [í•µì‹¬ ë¡œì§] ì¶©ë¶„íˆ ëª¨ì€ í›„ ëœë¤ìœ¼ë¡œ ì„ì–´ì„œ 30ê°œë§Œ ìë¦„ (ë‹¤ì–‘ì„± í™•ë³´)
    if len(candidate_jobs) > FINAL_TARGET_COUNT:
        print(f"âœ‚ï¸ ëª©í‘œ ìˆ˜ëŸ‰({FINAL_TARGET_COUNT}ê°œ)ì— ë§ì¶° ëœë¤ ì„ ë³„ ì¤‘...")
        random.shuffle(candidate_jobs)
        final_jobs = candidate_jobs[:FINAL_TARGET_COUNT]
    else:
        final_jobs = candidate_jobs

    # ID ì¬ë¶€ì—¬
    for idx, job in enumerate(final_jobs):
        job['id'] = idx + 1
        
    return final_jobs

# --- ì‹¤í–‰ ë° íŒŒì¼ ì €ì¥ ---
if __name__ == "__main__":
    jobs = collect_private_jobs_by_size()
    
    if jobs:
        # 1. JOBS í´ë” ìƒì„±
        save_dir = "JOBS"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        # 2. íŒŒì¼ ì €ì¥
        save_path = os.path.join(save_dir, "recruit_data.json")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=4)
            
        print("\n" + "="*50)
        print(f"ğŸ‰ ìµœì¢… ì„ ë³„ëœ ì•Œì§œ ê³µê³  {len(jobs)}ê°œ ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}")
        print("="*50)
    else:
        print("\nğŸ’€ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")