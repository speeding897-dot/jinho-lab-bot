import os
import requests
from bs4 import BeautifulSoup
import json
import random
import time

# ==========================================
# 1. ì„¤ì • (ê²€ìƒ‰ì–´ ì—†ìŒ! ê·œëª¨ë³„ URL íƒ€ê²ŸíŒ…)
# ==========================================
# ì¸í¬ë£¨íŠ¸ ê¸°ì—…ë¶„ë¥˜ë³„ URL ë¦¬ìŠ¤íŠ¸ (ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…, ê°•ì†Œê¸°ì—…)
TARGET_URLS = [
    # ëŒ€ê¸°ì—… (Group & Big)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=1", 
    # ì¤‘ê²¬ê¸°ì—… (Mid-sized)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=2", 
    # ê°•ì†Œê¸°ì—…/íˆë“ ì±”í”¼ì–¸ (Hidden Champion)
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=3"
]

# ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

# í˜¹ì‹œ ì„ì—¬ ìˆì„ ê³µê¸°ì—…/ê³µë¬´ì› í•„í„°ë§
EXCLUDE_KEYWORDS = ["ê³µì‚¬", "ê³µë‹¨", "ì¬ë‹¨", "í˜‘íšŒ", "ì§„í¥ì›", "ì‹œì²­", "êµ¬ì²­", "ì„¼í„°", "ê³µë¬´ì›", "ë³´ê±´ì†Œ"]

# â˜… ìµœì¢…ì ìœ¼ë¡œ ì €ì¥í•  ê³µê³  ê°œìˆ˜ ëª©í‘œ
FINAL_TARGET_COUNT = 30

def collect_private_jobs_by_size():
    print(f"ğŸ”¥ [ì¸í¬ë£¨íŠ¸] ê´‘ë²”ìœ„ ìˆ˜ì§‘ ëª¨ë“œ ê°€ë™ (í˜ì´ì§€ ë„˜ê¹€ ê¸°ëŠ¥ ì¶”ê°€ë¨)...")
    
    candidate_jobs = [] # í›„ë³´êµ°ì„ ë‹´ì„ ì„ì‹œ ë¦¬ìŠ¤íŠ¸
    
    # ê° ê¸°ì—… ê·œëª¨ë³„ URLì„ ìˆœíšŒ
    for base_url in TARGET_URLS:
        
        # [ìˆ˜ì •] ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 1í˜ì´ì§€ë¶€í„° 5í˜ì´ì§€ê¹Œì§€ íƒìƒ‰ (Pagination)
        for page in range(1, 6):
            try:
                # ëª©í‘œëŸ‰ì˜ 3ë°° ì´ìƒ ëª¨ì˜€ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (ì†ë„ ìµœì í™”)
                if len(candidate_jobs) >= FINAL_TARGET_COUNT * 3:
                    break

                # URLì— í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€ (&page=1, &page=2 ...)
                target_url = f"{base_url}&page={page}"
                print(f"   Targeting URL: {target_url} (Page {page})...")
                
                response = requests.get(target_url, headers=HEADERS, timeout=10)
                response.encoding = response.apparent_encoding 
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì¸í¬ë£¨íŠ¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ
                all_list_items = soup.select('ul.c_list > li')
                
                # í•´ë‹¹ í˜ì´ì§€ì— ê³µê³ ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°
                if not all_list_items:
                    print(f"      ã„´ ê³µê³  ì—†ìŒ. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™.")
                    break

                for item in all_list_items:
                    try:
                        # 1. íšŒì‚¬ëª… ì¶”ì¶œ & í•„í„°ë§
                        comp_tag = item.find(class_='cpname')
                        if not comp_tag: continue
                        company = comp_tag.get_text(strip=True)

                        # ê³µê¸°ì—… í‚¤ì›Œë“œ í•„í„°ë§
                        if any(k in company for k in EXCLUDE_KEYWORDS):
                            continue

                        # 2. ì œëª© & ë§í¬ ì¶”ì¶œ
                        title_tag = item.find(class_='cl_top') or item.find(class_='hdit')
                        if not title_tag: continue
                        
                        link_tag = title_tag.find('a')
                        if not link_tag: continue

                        title = link_tag.get_text(strip=True)
                        link = link_tag['href']
                        if link.startswith("/"):
                            link = "https://job.incruit.com" + link

                        # 3. ê¸°ì—… í˜•íƒœ(ê·œëª¨) íƒœê·¸ ì¶”ì¶œ
                        type_tags = []
                        for icon in item.find_all(class_='icon'):
                            tag_text = icon.get_text(strip=True)
                            if tag_text: type_tags.append(tag_text)
                        
                        if "cd=1" in base_url and "ëŒ€ê¸°ì—…" not in type_tags: type_tags.insert(0, "ëŒ€ê¸°ì—…")
                        elif "cd=2" in base_url and "ì¤‘ê²¬ê¸°ì—…" not in type_tags: type_tags.insert(0, "ì¤‘ê²¬ê¸°ì—…")
                        elif "cd=3" in base_url and "ê°•ì†Œê¸°ì—…" not in type_tags: type_tags.insert(0, "ê°•ì†Œê¸°ì—…")
                        
                        type_str = ", ".join(type_tags)

                        # 4. ì„¸ë¶€ ì •ë³´ (ê²½ë ¥, í•™ë ¥ ë“±)
                        details = item.find_all('span')
                        exp = "ë¬´ê´€"
                        edu = "ë¬´ê´€"
                        
                        info_texts = [d.get_text(strip=True) for d in details if len(d.get_text(strip=True)) > 1]
                        for text in info_texts:
                            if "ê²½ë ¥" in text or "ì‹ ì…" in text: exp = text
                            elif "ëŒ€ì¡¸" in text or "ê³ ì¡¸" in text or "í•™ë ¥" in text: edu = text

                        # 5. ë§ˆê°ì¼ ì¶”ì¶œ
                        deadline_tag = item.find(class_='cl_btm')
                        deadline = "ì±„ìš©ì‹œ"
                        if deadline_tag:
                            d_span = deadline_tag.find('span')
                            if d_span: deadline = d_span.get_text(strip=True)

                        # 6. ë°ì´í„° ë‹´ê¸° (IDëŠ” ë‚˜ì¤‘ì— ë¶€ì—¬)
                        job_data = {
                            "company": company,
                            "type": type_str,
                            "title": title,
                            "exp": exp,
                            "edu": edu,
                            "deadline": deadline,
                            "link": link
                        }
                        
                        candidate_jobs.append(job_data)

                    except Exception:
                        continue 
                
                # í•œ í˜ì´ì§€ ê¸ì€ í›„ ì ì‹œ ëŒ€ê¸° (ì°¨ë‹¨ ë°©ì§€)
                time.sleep(1) 

            except Exception as e:
                print(f"   âŒ í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜: {e}")
                continue

    print(f"\nğŸ“Š ìˆ˜ì§‘ ì¢…ë£Œ: ì´ {len(candidate_jobs)}ê°œì˜ í›„ë³´ ê³µê³  í™•ë³´")
    
    # [í•µì‹¬ ë¡œì§] ì¶©ë¶„íˆ ëª¨ì€ í›„ ëœë¤ìœ¼ë¡œ ì„ì–´ì„œ 30ê°œë§Œ ìë¦„ (ë‹¤ì–‘ì„± í™•ë³´)
    if len(candidate_jobs) > FINAL_TARGET_COUNT:
        print(f"âœ‚ï¸ ëª©í‘œ ìˆ˜ëŸ‰({FINAL_TARGET_COUNT}ê°œ)ì— ë§ì¶° ëœë¤ ì„ ë³„ ì¤‘...")
        random.shuffle(candidate_jobs)
        final_jobs = candidate_jobs[:FINAL_TARGET_COUNT]
    else:
        final_jobs = candidate_jobs

    # ID ì¬ë¶€ì—¬
    for idx, job in enumerate(final_jobs):
        job['id'] = idx + 1
        
    return final_jobs

# --- ì‹¤í–‰ ë° íŒŒì¼ ì €ì¥ ---
if __name__ == "__main__":
    jobs = collect_private_jobs_by_size()
    
    if jobs:
        # 1. JOBS í´ë” ìƒì„±
        save_dir = "JOBS"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        # 2. íŒŒì¼ ì €ì¥
        save_path = os.path.join(save_dir, "recruit_data.json")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(jobs, f, ensure_ascii=False, indent=4)
            
        print("\n" + "="*50)
        print(f"ğŸ‰ ìµœì¢… ì„ ë³„ëœ ì•Œì§œ ê³µê³  {len(jobs)}ê°œ ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}")
        print("="*50)
    else:
        print("\nğŸ’€ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")