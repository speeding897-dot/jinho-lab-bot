import os
import requests
from bs4 import BeautifulSoup
import json
import random
import time

# ==========================================
# 1. ì„¤ì • (ì¸í¬ë£¨íŠ¸ ìµœì‹  HTML êµ¬ì¡° ë°˜ì˜)
# ==========================================
TARGET_URLS = [
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=1", # ëŒ€ê¸°ì—…
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=2", # ì¤‘ê²¬ê¸°ì—…
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=3"  # ê°•ì†Œê¸°ì—…
]

# [í•µì‹¬] ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë” (Referer ì¶”ê°€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Referer": "https://job.incruit.com/",
    "Connection": "keep-alive"
}

EXCLUDE_KEYWORDS = ["ê³µì‚¬", "ê³µë‹¨", "ì¬ë‹¨", "í˜‘íšŒ", "ì§„í¥ì›", "ì‹œì²­", "êµ¬ì²­", "ì„¼í„°", "ê³µë¬´ì›", "ë³´ê±´ì†Œ"]
FINAL_TARGET_COUNT = 30

def collect_private_jobs_by_size():
    print(f"ğŸ”¥ [Collector] ì¤‘ë³µ ì œê±° ë° ì™„ì „ ìˆ˜ì§‘ ëª¨ë“œ ì‹œì‘ (ëª©í‘œ: {FINAL_TARGET_COUNT}ê°œ)...")
    
    candidate_jobs = []
    seen_links = set() # â˜… [ì¶”ê°€] ì¤‘ë³µ ê³µê³  ë°©ì§€ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸
    
    for base_url in TARGET_URLS:
        # í˜ì´ì§€ íƒìƒ‰ (1~3í˜ì´ì§€)
        for page in range(1, 4):
            try:
                # ëª©í‘œëŸ‰ì˜ 4ë°°ìˆ˜ ì´ìƒ ëª¨ì´ë©´ ì¤‘ë‹¨ (í•„í„°ë§ ê³ ë ¤ ë„‰ë„‰í•˜ê²Œ)
                if len(candidate_jobs) >= FINAL_TARGET_COUNT * 4: break

                target_url = f"{base_url}&page={page}"
                print(f"   ğŸ“¡ ì ‘ì†: {target_url} ... ", end="")
                
                response = requests.get(target_url, headers=HEADERS, timeout=10)
                response.encoding = response.apparent_encoding 
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # â˜…â˜…â˜… [ìˆ˜ì • í•µì‹¬] ìƒë‹¨(Premium) + í•˜ë‹¨(General) ëª¨ë‘ ìˆ˜ì§‘ â˜…â˜…â˜…
                # ê¸°ì¡´ ì½”ë“œì˜ 'if not list_area' ë¡œì§ì„ ì‚­ì œí•˜ê³  ë‘˜ ë‹¤ ê°€ì ¸ì™€ì„œ í•©ì¹©ë‹ˆë‹¤.
                list_premium = soup.select('div.cPrdlists_rows div.cPrdlists_cols') # ìƒë‹¨
                list_general = soup.select('div.cBbslist_contenst ul.c_row')        # í•˜ë‹¨
                
                # ë‘ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ëˆ„ë½ ë°©ì§€)
                all_items = list_premium + list_general
                
                if not all_items:
                    print("âŒ ê³µê³  ëª» ì°¾ìŒ (êµ¬ì¡°ê°€ ë‹¤ë¥´ê±°ë‚˜ ì°¨ë‹¨ë¨)")
                    # ë””ë²„ê¹…ìš©: í˜ì´ì§€ ì œëª© ì¶œë ¥
                    print(f"      ã„´ í˜ì´ì§€ ì œëª©: {soup.title.text.strip() if soup.title else 'ì—†ìŒ'}")
                    continue 
                else:
                    print(f"âœ… {len(all_items)}ê°œ ë°œê²¬ (ìƒë‹¨:{len(list_premium)} + í•˜ë‹¨:{len(list_general)})")

                for item in all_items:
                    try:
                        # 1. íšŒì‚¬ëª… (íƒœê·¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë‘ ê°€ì§€ ë‹¤ í™•ì¸)
                        comp_tag = item.select_one('.cpname') or item.select_one('.cCpName')
                        if not comp_tag: continue
                        company = comp_tag.get_text(strip=True)

                        if any(k in company for k in EXCLUDE_KEYWORDS): continue

                        # 2. ì œëª© & ë§í¬ 
                        # ìƒë‹¨/í•˜ë‹¨ êµ¬ì¡° ì°¨ì´ ëŒ€ì‘
                        title_tag = item.select_one('.cell_mid .cl_top a') or item.select_one('.cTitle strong') or item.select_one('.cTitle') or item.select_one('.cl_top a')
                        
                        if not title_tag: continue

                        title = title_tag.get_text(strip=True)
                        
                        # ë§í¬ ì¶”ì¶œ (a íƒœê·¸ê°€ ìˆëŠ” ìƒìœ„ ìš”ì†Œ ì°¾ê¸°)
                        link_tag = item.find('a', href=True)
                        # ì œëª© íƒœê·¸ ìì²´ê°€ aíƒœê·¸ì¸ ê²½ìš°
                        if title_tag.name == 'a': link_tag = title_tag
                        
                        if not link_tag: continue
                        link = link_tag['href']
                        if link.startswith("/"): link = "https://job.incruit.com" + link

                        # â˜… [ì¶”ê°€] ì¤‘ë³µ ë°©ì§€ ë¡œì§
                        if link in seen_links:
                            continue # ì´ë¯¸ ìˆ˜ì§‘í•œ ë§í¬ë©´ íŒ¨ìŠ¤
                        seen_links.add(link)

                        # 3. ë§ˆê°ì¼
                        deadline = "ì±„ìš©ì‹œ"
                        # í•˜ë‹¨í˜• êµ¬ì¡°
                        d_tag = item.select_one('.cell_last .cl_btm span:first-child')
                        # ìƒë‹¨í˜• êµ¬ì¡° (.cDate)
                        if not d_tag: d_tag = item.select_one('.cDate')
                        
                        if d_tag: deadline = d_tag.get_text(strip=True)

                        # 4. ì €ì¥í•  ë°ì´í„° êµ¬ì„±
                        job_data = {
                            "company": company,
                            "title": title,
                            "link": link,
                            "deadline": deadline,
                            "id": 0 # ë‚˜ì¤‘ì— ì¼ê´„ ë¶€ì—¬
                        }
                        candidate_jobs.append(job_data)

                    except Exception:
                        continue
                
                time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ ëŒ€ê¸°

            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {e}")
                continue

    print(f"\nğŸ“Š [ìµœì¢… ê²°ê³¼] ì¤‘ë³µ ì œê±° í›„ í™•ë³´ëœ ê³µê³ : {len(candidate_jobs)}ê±´")
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹„ìƒ ê²½ê³ 
    if not candidate_jobs:
        print("ğŸš¨ [ë¹„ìƒ] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê±´ì…ë‹ˆë‹¤. HTML êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
        return []

    # ì…”í”Œ ë° 30ê°œ ìë¥´ê¸°
    random.shuffle(candidate_jobs)
    final_jobs = candidate_jobs[:FINAL_TARGET_COUNT]

    # ID ë¶€ì—¬
    for idx, job in enumerate(final_jobs):
        job['id'] = idx + 1
        
    return final_jobs

if __name__ == "__main__":
    jobs = collect_private_jobs_by_size()
    
    # í´ë” ìƒì„± (í˜„ì¬ ìœ„ì¹˜ ê¸°ì¤€)
    save_dir = "JOBS"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # íŒŒì¼ ì €ì¥
    save_path = os.path.join(save_dir, "recruit_data.json")
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(jobs, f, ensure_ascii=False, indent=4)
        
    if jobs:
        print(f"ğŸ‰ recruit_data.json ì €ì¥ ì™„ë£Œ! ({len(jobs)}ê±´)")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {os.path.abspath(save_path)}")
    else:
        print("ğŸ’€ ë¹ˆ íŒŒì¼ ì €ì¥ë¨.")