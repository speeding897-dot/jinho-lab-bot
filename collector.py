import os
import requests
from bs4 import BeautifulSoup
import json
import random
import time

# ==========================================
# 1. ì„¤ì • (ì¸í¬ë£¨íŠ¸ ìµœì‹  HTML êµ¬ì¡° ë°˜ì˜)
# ==========================================
TARGET_URLS = [
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=1", # ëŒ€ê¸°ì—…
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=2", # ì¤‘ê²¬ê¸°ì—…
    "https://job.incruit.com/jobdb_list/searchjob.asp?ct=6&ty=1&cd=3"  # ê°•ì†Œê¸°ì—…
]

# [í•µì‹¬] ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë” (Referer ì¶”ê°€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Referer": "https://job.incruit.com/",
    "Connection": "keep-alive"
}

EXCLUDE_KEYWORDS = ["ê³µì‚¬", "ê³µë‹¨", "ì¬ë‹¨", "í˜‘íšŒ", "ì§„í¥ì›", "ì‹œì²­", "êµ¬ì²­", "ì„¼í„°", "ê³µë¬´ì›", "ë³´ê±´ì†Œ"]
FINAL_TARGET_COUNT = 30

def collect_private_jobs_by_size():
    print(f"ğŸ”¥ [Collector] ì¸í¬ë£¨íŠ¸ ì •ë°€ ìˆ˜ì§‘ ì‹œì‘ (íƒ€ê²Ÿ ìˆ˜ì •: c_row)...")
    
    candidate_jobs = []
    
    for base_url in TARGET_URLS:
        # í˜ì´ì§€ íƒìƒ‰ (1~3í˜ì´ì§€)
        for page in range(1, 4):
            try:
                # ëª©í‘œëŸ‰ì˜ 3ë°° ì´ìƒ ëª¨ì´ë©´ ì¤‘ë‹¨ (ì†ë„ ìµœì í™”)
                if len(candidate_jobs) >= FINAL_TARGET_COUNT * 3: break

                target_url = f"{base_url}&page={page}"
                print(f"   ğŸ“¡ ì ‘ì†: {target_url} ... ", end="")
                
                response = requests.get(target_url, headers=HEADERS, timeout=10)
                response.encoding = response.apparent_encoding 
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # â˜…â˜…â˜… [ìˆ˜ì • í•µì‹¬ 1] ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ ì˜ì—­ (ul.c_row) â˜…â˜…â˜…
                # ì†Œì¥ë‹˜ì´ ì£¼ì‹  HTML íŒŒì¼ì— ìˆëŠ” ì •í™•í•œ íƒœê·¸ ê²½ë¡œì…ë‹ˆë‹¤.
                list_area = soup.select('div.cBbslist_contenst ul.c_row')
                
                # â˜…â˜…â˜… [ìˆ˜ì • í•µì‹¬ 2] í”„ë¦¬ë¯¸ì—„ ê³µê³  ì˜ì—­ (ìƒë‹¨ ë°•ìŠ¤í˜•) â˜…â˜…â˜…
                # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•  ê²½ìš° ìƒë‹¨ í”„ë¦¬ë¯¸ì—„ ê³µê³ ë„ ê¸ì–´ì˜µë‹ˆë‹¤.
                if not list_area:
                    print("   âš ï¸ ì¼ë°˜ ëª©ë¡ ì—†ìŒ, ìƒë‹¨ í”„ë¦¬ë¯¸ì—„ ê³µê³  í™•ì¸ ì¤‘...")
                    list_area = soup.select('div.cPrdlists_rows div.cPrdlists_cols')

                if not list_area:
                    print("âŒ ê³µê³  ëª» ì°¾ìŒ (êµ¬ì¡°ê°€ ë‹¤ë¥´ê±°ë‚˜ ì°¨ë‹¨ë¨)")
                    # ë””ë²„ê¹…ìš©: í˜ì´ì§€ ì œëª© ì¶œë ¥
                    print(f"      ã„´ í˜ì´ì§€ ì œëª©: {soup.title.text.strip() if soup.title else 'ì—†ìŒ'}")
                    continue # ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°
                else:
                    print(f"âœ… {len(list_area)}ê°œ ë°œê²¬!")

                for item in list_area:
                    try:
                        # 1. íšŒì‚¬ëª… (class='cpname')
                        comp_tag = item.select_one('.cpname')
                        if not comp_tag: continue
                        company = comp_tag.get_text(strip=True)

                        if any(k in company for k in EXCLUDE_KEYWORDS): continue

                        # 2. ì œëª© & ë§í¬ 
                        # ì¼ë°˜í˜•(.cell_mid)ê³¼ ë°•ìŠ¤í˜•(.cTitle) êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë‘ ê°€ì§€ ë‹¤ ì²´í¬
                        title_tag = item.select_one('.cell_mid .cl_top a') or item.select_one('.cTitle strong') or item.select_one('.cl_top a')
                        
                        if not title_tag: continue

                        title = title_tag.get_text(strip=True)
                        
                        # ë§í¬ ì¶”ì¶œ (a íƒœê·¸ê°€ ìˆëŠ” ìƒìœ„ ìš”ì†Œ ì°¾ê¸°)
                        link_tag = item.find('a', href=True)
                        # ì œëª© íƒœê·¸ ìì²´ê°€ aíƒœê·¸ì¸ ê²½ìš°
                        if title_tag.name == 'a': link_tag = title_tag
                        
                        if not link_tag: continue
                        link = link_tag['href']
                        if link.startswith("/"): link = "https://job.incruit.com" + link

                        # 3. ë§ˆê°ì¼
                        deadline = "ì±„ìš©ì‹œ"
                        # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°: .cell_last ì•ˆì˜ ì²«ë²ˆì§¸ span
                        d_tag = item.select_one('.cell_last .cl_btm span:first-child')
                        # í”„ë¦¬ë¯¸ì—„ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°: .cDate
                        if not d_tag: d_tag = item.select_one('.cDate')
                        
                        if d_tag: deadline = d_tag.get_text(strip=True)

                        # 4. ì €ì¥í•  ë°ì´í„° êµ¬ì„±
                        job_data = {
                            "company": company,
                            "title": title,
                            "link": link,
                            "deadline": deadline,
                            "id": 0 # ë‚˜ì¤‘ì— ì¼ê´„ ë¶€ì—¬
                        }
                        candidate_jobs.append(job_data)

                    except Exception:
                        continue
                
                time.sleep(1) # ì°¨ë‹¨ ë°©ì§€ ëŒ€ê¸°

            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {e}")
                continue

    print(f"\nğŸ“Š [ìµœì¢… ê²°ê³¼] ìˆ˜ì§‘ëœ ë°ì´í„°: {len(candidate_jobs)}ê±´")
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹„ìƒ ê²½ê³ 
    if not candidate_jobs:
        print("ğŸš¨ [ë¹„ìƒ] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê±´ì…ë‹ˆë‹¤. HTML êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
        return []

    # ì…”í”Œ ë° 30ê°œ ìë¥´ê¸°
    random.shuffle(candidate_jobs)
    final_jobs = candidate_jobs[:FINAL_TARGET_COUNT]

    # ID ë¶€ì—¬
    for idx, job in enumerate(final_jobs):
        job['id'] = idx + 1
        
    return final_jobs

if __name__ == "__main__":
    jobs = collect_private_jobs_by_size()
    
    # í´ë” ìƒì„± (í˜„ì¬ ìœ„ì¹˜ ê¸°ì¤€)
    save_dir = "JOBS"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # íŒŒì¼ ì €ì¥
    save_path = os.path.join(save_dir, "recruit_data.json")
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(jobs, f, ensure_ascii=False, indent=4)
        
    if jobs:
        print(f"ğŸ‰ recruit_data.json ì €ì¥ ì™„ë£Œ! ({len(jobs)}ê±´)")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {os.path.abspath(save_path)}")
    else:
        print("ğŸ’€ ë¹ˆ íŒŒì¼ ì €ì¥ë¨.")